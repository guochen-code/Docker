-- unstructured text
-- clean up - NL techiniques
-- unsupervised approach -clustering

previous app is very targeted and domain oriented. How to build up a generic app that can take in any kind of data set as long as you get certain parameters right
you can give an output that can mak sense on any domain or on any kind of data

***************************************************************************************************************************************************************
from flask import Flask, request, make_response, send_file
from stemming.porter2 import stem # stem('affected') -> 'affect'
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from io import BytesIO
import time
import zipfile


app = Flask(__name__)

def cleanse_text(text):
  if text:
    # remove whitespaces
    clean = ' '.join(i for i in text.split())
    # stemming a word (same as lemmonization) # care, caring, coming down to same word, cat, cats -> cat
    red_text = [stem(word) for word in clean.split()]
    return ' '.join(red_text)
  else:
    return text

@app.route('/cluster',methods=['POST'])
def cluster():
  data = pd.read_csv(request.files['dataset'])
  unstructure = 'text'
  if 'col' in request.args:
    unstructure = request.args.get('col')
  no_of_clusters=2
  if 'no_of_clusters' in request.args:
    no_of_clusters=request.args.get('no_of_clusters')
  
  data = data.fillna('NULL')
  
  data['clean_sum'] = data[unstructure].apply(cleanse_text)
  
  vectorizer = CountVectorizer(analyzer='word', stop_words='english') # intelligent to split by word # the, an, a.....
  # doc1 a b c
  # doc2 b c d
  #
  #         a b c d
  # doc1    1 1 1 0
  # doc2    0 1 1 1
  counts=vectorizer.fit_transform(data['clean_sum'])
  
  kmeans = Kmeans(n_clusters=no_of_clusters)
  
  data['cluster_num'] = kmeans.fit_predict(counts)
  data = data.drop)['clean_sum'],axis=1)
  
  output = BytesIO()
  writer=pd.ExcelWrite(output,engine='xlsxwriter')
  data.to_excel(writer,sheet_name='Clusters',encoding='utf-8',index=False)
  
  # add statistics
  clusters=[]
  for i in range(np.shape(kmeans.cluster_centers_)[0]):
    data_cluster=pd.concat([pd.Series(vectorizer.get_feature_names(),pd.DataFrame(kmeans.cluster_centers_[i],axis=1)
    data_cluster.columns=['keywords','weights']
    data_cluster=data_cluster.sort_values(by=['weights'],ascending=False)
    data_clust=data_cluster.head(n=10)['keywords'].tolist()
    cluster.append(data_clust)
  pd.DataFrame(clusters).to_excel(writer,sheet_name='Top_Keywords',encoding='utf-8')
  
  #pivot
  data_pivot=data.groupby(['cluster_num'], as_index=False).size() # counts of different clusters
  data_pivot.name='size' # give the counts column a name
  data_pivot=data_pivot.reset_index()
  data_pivot.to_excel(writer,sheet_name='Cluster_Report',encoding='utf-8',index=False)
  # insert chart
  workbook=writer.book
  worksheet=writer.sheets['Cluster_Report']
  chart=workbook.add_chart({'type':'column'})
  chart.add_series({
                    'values': '=Cluster_Report!$B$2:$B'+str(no_of_clusters+1) # no_of_clusters+1=3 -> $B3
                    })
  worksheet.insert_chart('D2',chart)
   
  writer.save()
  
  memory_file=BytesIO()
  with zipfile.ZipFile(memory_file,'w') as zf:
    names = ['cluster_output.xlsx']
    files=[output] # [output,output2,ouput3]
    for i in range(len(files)):
      data = zipfile.ZipInfo(names[i])
      data.date_time=time.localtime(time.time())
      data.compress_type=zipfile.ZIP_DEFLATED
      zf.writestr(data,files[i],getvalue())
  memory_file.seek(0)
  response=make_response(send_file(memory_file, attachment_filename='cluster_output.zip', as_attachement=True))
  response.headers['Content-Disposition']='attachment;filename=cluster_output.zip')
  response.headers['Access-Control-Allow-Origin'] = '*' # cross origin request
  
  return response
  
if __name__ =='__main__':
  app.run(host='0.0.0.0')
  
  
******************************************* docker
List the library names that you included in requirements.txt

docker build and docker run should succesfully execute, the application open flawlessly from host machine's browser

Please make a pull request from this repo and try making an update - https://github.com/vivekkalyanarangan30/Text-Clustering-API/blob/master/Dockerfile

Were you able to write the Dockerfile with only one RUN command? If yes, share the command

docker build and docker run should succesfully execute, the application open flawlessly from host machine's browser

Please make a pull request from this repo and try making an update - https://github.com/vivekkalyanarangan30/Text-Clustering-API/blob/master/Dockerfile
  
